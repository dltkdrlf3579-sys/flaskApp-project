#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 7: JSON/ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” 
PostgreSQL Migration v7 - ì¸ë±ìŠ¤ ìƒì„± ë° ì„±ëŠ¥ íŠœë‹
"""
import sys
import os
import psycopg
import configparser
import time
import json
from typing import Dict, List, Optional, Tuple

# Windowsì—ì„œ í•œê¸€ ì¶œë ¥ì„ ìœ„í•œ ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

def get_config():
    """config.iniì—ì„œ DB ì„¤ì • ì½ê¸°"""
    config = configparser.ConfigParser()
    config_path = os.path.join(os.path.dirname(__file__), 'config.ini')
    
    if os.path.exists(config_path):
        config.read(config_path, encoding='utf-8')
        
        # PostgreSQL ì„¤ì • ì½ê¸°
        host = config.get('postgresql', 'host', fallback='localhost')
        port = config.get('postgresql', 'port', fallback='5432')
        database = config.get('postgresql', 'database', fallback='portal_dev')
        admin_user = config.get('postgresql', 'admin_user', fallback='postgres')
        admin_password = config.get('postgresql', 'admin_password', fallback='admin123')
        
        admin_dsn = f'postgresql://{admin_user}:{admin_password}@{host}:{port}/{database}'
        return admin_dsn
    else:
        return 'postgresql://postgres:admin123@localhost:5432/portal_dev'

class IndexAnalyzer:
    """ì¸ë±ìŠ¤ ë¶„ì„ ë° ì¶”ì²œ ë„êµ¬"""
    
    def __init__(self, dsn: str):
        self.dsn = dsn
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ë§Œ í™•ì¸
        self.target_tables = self._get_existing_tables()
    
    def _get_existing_tables(self) -> List[str]:
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ë§Œ ë°˜í™˜"""
        potential_tables = [
            'accidents_cache',
            'safety_instructions_cache', 
            'follow_sop',
            'full_process',
            'followsop_cache',
            'fullprocess_cache',
            'partner_change_requests'
        ]
        
        existing_tables = []
        try:
            with psycopg.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    for table in potential_tables:
                        cur.execute("""
                            SELECT EXISTS (
                                SELECT FROM information_schema.tables 
                                WHERE table_name = %s AND table_schema = 'public'
                            )
                        """, (table,))
                        
                        if cur.fetchone()[0]:
                            existing_tables.append(table)
        except Exception:
            pass
        
        return existing_tables
    
    def analyze_current_indexes(self) -> Dict[str, List]:
        """í˜„ì¬ ì¸ë±ìŠ¤ ìƒíƒœ ë¶„ì„"""
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                results = {}
                
                for table in self.target_tables:
                    # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables 
                            WHERE table_name = %s AND table_schema = 'public'
                        )
                    """, (table,))
                    
                    if not cur.fetchone()[0]:
                        print(f"âš ï¸  í…Œì´ë¸” {table}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                        continue
                    
                    # ì¸ë±ìŠ¤ ì¡°íšŒ
                    cur.execute("""
                        SELECT 
                            indexname,
                            indexdef,
                            schemaname
                        FROM pg_indexes 
                        WHERE tablename = %s AND schemaname = 'public'
                        ORDER BY indexname
                    """, (table,))
                    
                    indexes = cur.fetchall()
                    results[table] = [
                        {
                            'name': idx[0],
                            'definition': idx[1],
                            'schema': idx[2]
                        }
                        for idx in indexes
                    ]
                
                return results
    
    def analyze_json_keys(self) -> Dict[str, List[str]]:
        """ê° í…Œì´ë¸”ì˜ JSON í‚¤ ì‚¬ìš© ë¹ˆë„ ë¶„ì„"""
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                results = {}
                
                for table in self.target_tables:
                    # í…Œì´ë¸” ì¡´ì¬ ë° custom_data ì»¬ëŸ¼ í™•ì¸
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.columns 
                            WHERE table_name = %s 
                            AND column_name = 'custom_data'
                            AND table_schema = 'public'
                        )
                    """, (table,))
                    
                    if not cur.fetchone()[0]:
                        continue
                    
                    # JSON í‚¤ ì¶”ì¶œ (ìµœëŒ€ 1000ê°œ ìƒ˜í”Œ)
                    cur.execute(f"""
                        SELECT DISTINCT jsonb_object_keys(custom_data) as key_name, COUNT(*) as frequency
                        FROM (
                            SELECT custom_data 
                            FROM {table} 
                            WHERE custom_data IS NOT NULL 
                            AND jsonb_typeof(custom_data) = 'object'
                            LIMIT 1000
                        ) sample
                        GROUP BY jsonb_object_keys(custom_data)
                        ORDER BY frequency DESC
                        LIMIT 10
                    """)
                    
                    keys = [row[0] for row in cur.fetchall()]
                    results[table] = keys
                
                return results
    
    def get_table_stats(self) -> Dict[str, Dict]:
        """í…Œì´ë¸” í†µê³„ ì •ë³´"""
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                results = {}
                
                for table in self.target_tables:
                    try:
                        # í…Œì´ë¸” í¬ê¸° ë° í–‰ ìˆ˜
                        cur.execute(f"""
                            SELECT 
                                COUNT(*) as row_count,
                                pg_size_pretty(pg_total_relation_size('{table}')) as total_size
                            FROM {table}
                        """)
                        
                        row = cur.fetchone()
                        if row:
                            results[table] = {
                                'row_count': row[0],
                                'total_size': row[1]
                            }
                    except Exception as e:
                        print(f"âš ï¸  {table} í†µê³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                        continue
                
                return results

class PerformanceIndexCreator:
    """ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±ê¸°"""
    
    def __init__(self, dsn: str):
        self.dsn = dsn
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ë§Œ í™•ì¸
        self.existing_tables = self._get_existing_tables()
        
        # í…Œì´ë¸”ë³„ í•µì‹¬ ê²€ìƒ‰ í‚¤ (ì‹¤ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜)
        all_patterns = {
            'accidents_cache': ['workplace', 'accident_type', 'severity', 'department'],
            'safety_instructions_cache': ['workplace', 'violation_type', 'severity', 'inspector'],
            'follow_sop': ['workplace', 'process_type', 'status', 'department'],
            'full_process': ['workplace', 'process_name', 'status', 'department'],
            'followsop_cache': ['workplace', 'process_type', 'status'],
            'fullprocess_cache': ['workplace', 'process_name', 'status'],
            'partner_change_requests': ['requester_name', 'company_name', 'status', 'department']
        }
        
        # ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ë§Œ í•„í„°ë§
        self.key_patterns = {
            table: keys for table, keys in all_patterns.items() 
            if table in self.existing_tables
        }
    
    def _get_existing_tables(self) -> List[str]:
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ë§Œ ë°˜í™˜"""
        potential_tables = [
            'accidents_cache', 'safety_instructions_cache', 'follow_sop', 'full_process',
            'followsop_cache', 'fullprocess_cache', 'partner_change_requests'
        ]
        
        existing_tables = []
        try:
            with psycopg.connect(self.dsn) as conn:
                with conn.cursor() as cur:
                    for table in potential_tables:
                        cur.execute("""
                            SELECT EXISTS (
                                SELECT FROM information_schema.tables 
                                WHERE table_name = %s AND table_schema = 'public'
                            )
                        """, (table,))
                        
                        if cur.fetchone()[0]:
                            existing_tables.append(table)
        except Exception:
            pass
        
        return existing_tables
    
    def create_expression_indexes(self, table: str, keys: List[str]) -> List[str]:
        """í‘œí˜„ì‹ ì¸ë±ìŠ¤ ìƒì„± (ì •í™• ë§¤ì¹­ ìµœì í™”)"""
        created_indexes = []
        
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                for key in keys:
                    index_name = f"idx_{table}_{key}_expr"
                    
                    try:
                        # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
                        cur.execute("""
                            SELECT indexname FROM pg_indexes 
                            WHERE indexname = %s AND tablename = %s
                        """, (index_name, table))
                        
                        if cur.fetchone():
                            print(f"âœ… {index_name} ì´ë¯¸ ì¡´ì¬")
                            continue
                        
                        # í‘œí˜„ì‹ ì¸ë±ìŠ¤ ìƒì„±
                        create_sql = f"""
                            CREATE INDEX {index_name} 
                            ON {table} ((custom_data->>'{key}'))
                        """
                        
                        cur.execute(create_sql)
                        conn.commit()
                        
                        print(f"âœ… ìƒì„±: {index_name}")
                        created_indexes.append(index_name)
                        
                    except Exception as e:
                        print(f"âŒ {index_name} ìƒì„± ì‹¤íŒ¨: {e}")
                        continue
        
        return created_indexes
    
    def create_composite_indexes(self, table: str, key_combinations: List[Tuple[str, str]]) -> List[str]:
        """ë³µí•© ì¸ë±ìŠ¤ ìƒì„± (ë‹¤ì¤‘ ì¡°ê±´ ê²€ìƒ‰ ìµœì í™”)"""
        created_indexes = []
        
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                for key1, key2 in key_combinations:
                    index_name = f"idx_{table}_{key1}_{key2}_composite"
                    
                    try:
                        # ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
                        cur.execute("""
                            SELECT indexname FROM pg_indexes 
                            WHERE indexname = %s AND tablename = %s
                        """, (index_name, table))
                        
                        if cur.fetchone():
                            print(f"âœ… {index_name} ì´ë¯¸ ì¡´ì¬")
                            continue
                        
                        # ë³µí•© ì¸ë±ìŠ¤ ìƒì„±
                        create_sql = f"""
                            CREATE INDEX {index_name} 
                            ON {table} ((custom_data->>'{key1}'), (custom_data->>'{key2}'))
                        """
                        
                        cur.execute(create_sql)
                        conn.commit()
                        
                        print(f"âœ… ìƒì„±: {index_name}")
                        created_indexes.append(index_name)
                        
                    except Exception as e:
                        print(f"âŒ {index_name} ìƒì„± ì‹¤íŒ¨: {e}")
                        continue
        
        return created_indexes
    
    def create_gin_indexes(self, table: str, selective: bool = True) -> List[str]:
        """GIN ì¸ë±ìŠ¤ ìƒì„± (ê´‘ë²”ìœ„ JSON ê²€ìƒ‰ìš©)"""
        created_indexes = []
        
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                # í…Œì´ë¸” í¬ê¸° í™•ì¸ (GINì€ ëŒ€ìš©ëŸ‰ì—ë§Œ ì ìš©)
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                row_count = cur.fetchone()[0]
                
                # GIN ì¸ë±ìŠ¤ëŠ” ë” í° ë°ì´í„°ì—ì„œë§Œ íš¨ìš©ì„± ìˆìŒ
                gin_threshold = 10000  # codex ê¶Œì¥: ë” ë†’ì€ ì„ê³„ì¹˜
                if selective and row_count < gin_threshold:
                    print(f"ğŸ“Š {table}: í–‰ ìˆ˜ {row_count:,} < {gin_threshold:,}, GIN ì¸ë±ìŠ¤ ìƒëµ (ê´‘ë²”ìœ„ ê²€ìƒ‰ ì‹œì—ë§Œ ìœ íš¨)")
                    return created_indexes
                
                index_name = f"idx_{table}_custom_data_gin"
                
                try:
                    # ê¸°ì¡´ GIN ì¸ë±ìŠ¤ í™•ì¸
                    cur.execute("""
                        SELECT indexname FROM pg_indexes 
                        WHERE indexname = %s AND tablename = %s
                    """, (index_name, table))
                    
                    if cur.fetchone():
                        print(f"âœ… {index_name} ì´ë¯¸ ì¡´ì¬")
                        return created_indexes
                    
                    # GIN ì¸ë±ìŠ¤ ìƒì„±
                    create_sql = f"""
                        CREATE INDEX {index_name} 
                        ON {table} USING GIN (custom_data)
                    """
                    
                    cur.execute(create_sql)
                    conn.commit()
                    
                    print(f"âœ… ìƒì„±: {index_name} (í–‰ ìˆ˜: {row_count:,})")
                    created_indexes.append(index_name)
                    
                except Exception as e:
                    print(f"âŒ {index_name} ìƒì„± ì‹¤íŒ¨: {e}")
        
        return created_indexes

class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë„êµ¬"""
    
    def __init__(self, dsn: str):
        self.dsn = dsn
    
    def test_query_performance(self, table: str, queries: List[Dict]) -> List[Dict]:
        """ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
        results = []
        
        with psycopg.connect(self.dsn) as conn:
            with conn.cursor() as cur:
                for query_info in queries:
                    name = query_info['name']
                    sql = query_info['sql']
                    params = query_info.get('params', ())
                    
                    try:
                        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì¸¡ì •
                        times = []
                        for _ in range(5):
                            start_time = time.time()
                            cur.execute(sql, params)
                            result = cur.fetchall()
                            end_time = time.time()
                            times.append(end_time - start_time)
                        
                        avg_time = sum(times) / len(times)
                        
                        # ì‹¤í–‰ ê³„íš ì¡°íšŒ
                        explain_sql = f"EXPLAIN (FORMAT JSON) {sql}"
                        cur.execute(explain_sql, params)
                        plan = cur.fetchone()[0][0]
                        
                        results.append({
                            'table': table,
                            'name': name,
                            'avg_time_ms': avg_time * 1000,
                            'result_count': len(result),
                            'execution_plan': plan,
                            'uses_index': 'Index' in str(plan)
                        })
                        
                    except Exception as e:
                        print(f"âŒ {name} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                        continue
        
        return results
    
    def generate_test_queries(self, table: str, keys: List[str]) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±"""
        queries = []
        
        # ì •í™• ë§¤ì¹­ í…ŒìŠ¤íŠ¸
        if 'workplace' in keys:
            queries.append({
                'name': f'{table} - ì •í™• ë§¤ì¹­ (workplace)',
                'sql': f"SELECT COUNT(*) FROM {table} WHERE custom_data->>'workplace' = %s",
                'params': ('ê³µì¥A',)
            })
        
        # LIKE ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if 'workplace' in keys:
            queries.append({
                'name': f'{table} - LIKE ê²€ìƒ‰ (workplace)',
                'sql': f"SELECT COUNT(*) FROM {table} WHERE custom_data->>'workplace' LIKE %s",
                'params': ('%ê³µì¥%',)
            })
        
        # ë³µí•© ì¡°ê±´ í…ŒìŠ¤íŠ¸ (ì•ˆì „í•œ SQL í¬ë§·)
        if len(keys) >= 2:
            key1, key2 = keys[0], keys[1]
            queries.append({
                'name': f'{table} - ë³µí•© ì¡°ê±´ ({key1} + {key2})',
                'sql': f"SELECT COUNT(*) FROM {table} WHERE custom_data->>'{key1}' = %s AND custom_data->>'{key2}' IS NOT NULL",
                'params': ('ê³µì¥A',)  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê°’ ì‚¬ìš©
            })
        
        # JSON í‚¤ ì¡´ì¬ í™•ì¸
        if keys:
            queries.append({
                'name': f'{table} - í‚¤ ì¡´ì¬ í™•ì¸ ({keys[0]})',
                'sql': f"SELECT COUNT(*) FROM {table} WHERE custom_data ? %s",
                'params': (keys[0],)
            })
        
        return queries

def analyze_current_state():
    """í˜„ì¬ ìƒíƒœ ë¶„ì„"""
    print("=== Phase 7: JSON/ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì‹œì‘ ===")
    
    dsn = get_config()
    analyzer = IndexAnalyzer(dsn)
    
    print("\n--- í˜„ì¬ ì¸ë±ìŠ¤ ìƒíƒœ ---")
    current_indexes = analyzer.analyze_current_indexes()
    for table, indexes in current_indexes.items():
        print(f"\nğŸ“Š {table}:")
        if indexes:
            for idx in indexes:
                print(f"  âœ… {idx['name']}")
        else:
            print("  âš ï¸  ì¸ë±ìŠ¤ ì—†ìŒ")
    
    print("\n--- JSON í‚¤ ë¶„ì„ ---")
    json_keys = analyzer.analyze_json_keys()
    for table, keys in json_keys.items():
        if keys:
            print(f"ğŸ“Š {table}: {', '.join(keys[:5])}")
    
    print("\n--- í…Œì´ë¸” í†µê³„ ---")
    table_stats = analyzer.get_table_stats()
    for table, stats in table_stats.items():
        print(f"ğŸ“Š {table}: {stats['row_count']:,}í–‰, {stats['total_size']}")
    
    return current_indexes, json_keys, table_stats

def create_performance_indexes():
    """ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„±"""
    print("\n=== ì„±ëŠ¥ ì¸ë±ìŠ¤ ìƒì„± ===")
    
    dsn = get_config()
    creator = PerformanceIndexCreator(dsn)
    
    total_created = []
    
    for table, keys in creator.key_patterns.items():
        print(f"\n--- {table} ìµœì í™” ---")
        
        # í‘œí˜„ì‹ ì¸ë±ìŠ¤ ìƒì„± (ìƒìœ„ 2ê°œ í‚¤ë§Œìœ¼ë¡œ ì œí•œ - codex ê¶Œì¥)
        print("1. í‘œí˜„ì‹ ì¸ë±ìŠ¤:")
        priority_keys = keys[:2]  # ì¸ë±ìŠ¤ í­ì¦ ë°©ì§€
        print(f"   ëŒ€ìƒ í‚¤: {priority_keys}")
        expr_indexes = creator.create_expression_indexes(table, priority_keys)
        total_created.extend(expr_indexes)
        
        # ë³µí•© ì¸ë±ìŠ¤ ìƒì„± (ì£¼ìš” ì¡°í•©ë§Œ)
        if len(keys) >= 2:
            print("2. ë³µí•© ì¸ë±ìŠ¤:")
            composite_combinations = [(keys[0], keys[1])]  # workplace + type/status ì¡°í•©
            comp_indexes = creator.create_composite_indexes(table, composite_combinations)
            total_created.extend(comp_indexes)
        
        # GIN ì¸ë±ìŠ¤ (ì„ ë³„ì )
        print("3. GIN ì¸ë±ìŠ¤:")
        gin_indexes = creator.create_gin_indexes(table, selective=True)
        total_created.extend(gin_indexes)
    
    # ì¸ë±ìŠ¤ ìƒì„± í›„ í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸ (codex ê¶Œì¥)
    if total_created:
        print(f"\n--- í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸ ---")
        try:
            with psycopg.connect(dsn) as conn:
                with conn.cursor() as cur:
                    cur.execute("ANALYZE")
                    print("âœ… ì „ì²´ í…Œì´ë¸” í†µê³„ ì •ë³´ ê°±ì‹  ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nâœ… ì´ {len(total_created)}ê°œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    return total_created

def test_performance_improvements():
    """ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ì¸¡ì •"""
    print("\n=== ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ì¸¡ì • ===")
    
    dsn = get_config()
    tester = PerformanceTester(dsn)
    creator = PerformanceIndexCreator(dsn)
    
    all_results = []
    
    for table, keys in creator.key_patterns.items():
        print(f"\n--- {table} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ---")
        
        queries = tester.generate_test_queries(table, keys)
        results = tester.test_query_performance(table, queries)
        
        for result in results:
            print(f"ğŸ“Š {result['name']}: {result['avg_time_ms']:.2f}ms "
                  f"({result['result_count']}ê±´) "
                  f"{'ğŸš€ ì¸ë±ìŠ¤' if result['uses_index'] else 'ğŸŒ í’€ìŠ¤ìº”'}")
        
        all_results.extend(results)
    
    return all_results

def generate_performance_report(results: List[Dict]):
    """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
    report_path = os.path.join(os.path.dirname(__file__), 'PHASE7_PERFORMANCE_REPORT.md')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# Phase 7 ì„±ëŠ¥ ìµœì í™” ë³´ê³ ì„œ\n\n")
        f.write("## ğŸ“Š ì‹¤í–‰ ìš”ì•½\n\n")
        f.write(f"- í…ŒìŠ¤íŠ¸ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"- ì´ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ\n")
        
        indexed_queries = [r for r in results if r['uses_index']]
        f.write(f"- ì¸ë±ìŠ¤ í™œìš©: {len(indexed_queries)}/{len(results)}ê°œ\n\n")
        
        f.write("## ğŸ¯ ì„±ëŠ¥ ê²°ê³¼\n\n")
        
        # í…Œì´ë¸”ë³„ ê²°ê³¼
        tables = set(r['table'] for r in results)
        for table in sorted(tables):
            table_results = [r for r in results if r['table'] == table]
            if not table_results:
                continue
                
            f.write(f"### {table}\n\n")
            f.write("| ì¿¼ë¦¬ ìœ í˜• | ì‘ë‹µì‹œê°„ | ê²°ê³¼ ìˆ˜ | ì¸ë±ìŠ¤ í™œìš© |\n")
            f.write("|----------|---------|---------|-------------|\n")
            
            for result in table_results:
                index_status = "âœ…" if result['uses_index'] else "âŒ"
                f.write(f"| {result['name'].split(' - ')[1]} | "
                       f"{result['avg_time_ms']:.2f}ms | "
                       f"{result['result_count']:,} | "
                       f"{index_status} |\n")
            
            f.write("\n")
        
        f.write("## ğŸ“ˆ ìµœì í™” ê¶Œì¥ì‚¬í•­\n\n")
        
        slow_queries = [r for r in results if r['avg_time_ms'] > 100]  # 100ms ì´ˆê³¼
        if slow_queries:
            f.write("### ğŸŒ ëŠë¦° ì¿¼ë¦¬ (100ms ì´ˆê³¼)\n\n")
            for query in slow_queries:
                f.write(f"- **{query['name']}**: {query['avg_time_ms']:.2f}ms\n")
                if not query['uses_index']:
                    f.write("  - ê¶Œì¥: ì¶”ê°€ ì¸ë±ìŠ¤ ìƒì„± ê²€í† \n")
                f.write("\n")
        
        no_index_queries = [r for r in results if not r['uses_index']]
        if no_index_queries:
            f.write("### ğŸ“‹ ì¸ë±ìŠ¤ ë¯¸í™œìš© ì¿¼ë¦¬\n\n")
            for query in no_index_queries:
                f.write(f"- **{query['name']}**: í’€ í…Œì´ë¸” ìŠ¤ìº”\n")
            f.write("\n")
        
        f.write("## âœ… ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€\n\n")
        f.write("- ğŸ¯ ëª©í‘œ: ì£¼ìš” ê²€ìƒ‰ ì¿¼ë¦¬ < 50ms\n")
        
        fast_queries = [r for r in results if r['avg_time_ms'] <= 50]
        f.write(f"- âœ… ê¸°ì¤€ ë‹¬ì„±: {len(fast_queries)}/{len(results)}ê°œ ì¿¼ë¦¬\n")
        
        if len(fast_queries) == len(results):
            f.write("\nğŸ‰ **ëª¨ë“  ì¿¼ë¦¬ê°€ ì„±ëŠ¥ ê¸°ì¤€ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!**\n")
        else:
            f.write(f"\nâš ï¸  {len(results) - len(fast_queries)}ê°œ ì¿¼ë¦¬ê°€ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬ì„±\n")
    
    print(f"\nğŸ“‹ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±: {report_path}")
    return report_path

def main():
    """Phase 7 ë©”ì¸ ì‹¤í–‰"""
    try:
        # 1. í˜„ì¬ ìƒíƒœ ë¶„ì„
        current_indexes, json_keys, table_stats = analyze_current_state()
        
        # 2. ì„±ëŠ¥ ì¸ë±ìŠ¤ ìƒì„±
        created_indexes = create_performance_indexes()
        
        # 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        performance_results = test_performance_improvements()
        
        # 4. ë³´ê³ ì„œ ìƒì„±
        report_path = generate_performance_report(performance_results)
        
        # 5. ê²°ê³¼ ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ‰ Phase 7: JSON/ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ!")
        print("="*60)
        print(f"âœ… ìƒì„±ëœ ì¸ë±ìŠ¤: {len(created_indexes)}ê°œ")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ëœ ì¿¼ë¦¬: {len(performance_results)}ê°œ")
        
        indexed_count = sum(1 for r in performance_results if r['uses_index'])
        print(f"ğŸš€ ì¸ë±ìŠ¤ í™œìš©ë¥ : {indexed_count}/{len(performance_results)}ê°œ")
        
        fast_count = sum(1 for r in performance_results if r['avg_time_ms'] <= 50)
        print(f"âš¡ ì„±ëŠ¥ ê¸°ì¤€ ë‹¬ì„±: {fast_count}/{len(performance_results)}ê°œ (< 50ms)")
        
        print(f"ğŸ“‹ ìƒì„¸ ë³´ê³ ì„œ: {os.path.basename(report_path)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Phase 7 ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)